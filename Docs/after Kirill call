Ожидаемый объем данных:

число чеков - 1 2 миллиона в месяц
число позиций в чеке - 20 30 в среднем


													OVERALL REVIEW
													
Необходимость в Elastic:

1) быстрый доступ к внутрекухонным вещам типа меж айтемных расстояний, handle статистик итд
2) возможность горизонтального  масштабирования
3) необходимость в переносе данных на Postgre с elastic, банально на случай пиздеца и потери данных, в случае быстрых расчетов
можно скипнуть, и просто бэкапить сторэдж эластика каждые 3-5 дней

												    Spark vs Airflow

По всей видимости компэрисон двух этих технологий не о чем не говорит это параллельные друг другу технологии. Ибо:

Airflow - (tasker, scheduler) он просто выполняет планирование и постройку зависимостей между задачами. Не привязан к языку, на нем можно запускать
что угодно к примеру case запуска Spark джобов через Airflow scheduler, в кейсе описано написание трех взаимодействующих тасков,первый качает данные 
с амазоновской s3 два других представляет собой Spark Джобы по обработке скачанных данных, https://blog.insightdatascience.com/scheduling-spark-jobs-with-airflow-4c66f3144660, 
Еще один пример использования Airflow+Spark https://medium.com/swlh/building-a-big-data-pipeline-with-airflow-spark-and-zeppelin-843f31ef220c 
Пример аналогичный, Airflow выстраиывает логику и порядок работы задач, Spark со всей своей мощностью распределенно колбасит данные.
													
Большой плюс Airflow - это DAG, он описывается как код и генерятся динамически, например (кейс из IICT проекта, нужно предобработать
длинный датасэт, мы генерим условно 15 тасков которые паралелльно покусочно нам этот датасэт почистят/предобработают)

Для теста запустили расчет w2v векторов на 10 000 000 объектах из датасета Amazon (сопоставить с числом чеков и их длиной) 
CPU times: user 5min 9s, sys: 2.92 s, total: 5min 12s
Wall time: 1min 34s

Возможно такое, что на боевых данных, вектора будут считаться довольно быстро, следовательно наших мощностей хватит за глаза, кластер с нодами поверх 
которого будет spark не нужен


Spark, тоже порой используется как своего рода ETL движок, в контексте текущей задачи действительно имеет место быть с учетом факторов:

1) более развитое коммьюнити
2) двух хард-скилл ребят из Москвы, которые в перспективе в случае чего подставят плечо
3) возможность масштабирования в будущем

Но в противовес, выступают такие моменты как:

1) видимо это будет не pyspark, а scala или java, с которой для нас конкретно, задача становится сложнее
2) нулевое или практически нулевое представление написания тех-самых пайп лайн джобов
3) автоматически возникающая необходимость привлечения специалистов из москвы как минимум на колы, для уяснения множества деталей и моментов 







